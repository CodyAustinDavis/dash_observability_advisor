CREATE SCHEMA IF NOT EXISTS dash_observability_advisor;


CREATE TABLE IF NOT EXISTS alerts_settings (
    id BIGINT GENERATED BY DEFAULT AS IDENTITY,
    alert_name STRING,
    alert_query STRING,
    alert_schedule STRING,
    alert_recipients STRING,
    query_id STRING,
    alert_id STRING,
    job_id STRING,
    alert_column STRING,
    alert_condition STRING,
    alert_condition_value STRING
) CLUSTER BY (alert_name, alert_query);


CREATE MATERIALIZED VIEW IF NOT EXISTS clean_usage
TBLPROPERTIES('pipelines.autoOptimize.zOrderCols' = 'usage_start_time,billing_origin_product')
AS
WITH compute AS (
SELECT
  c.cluster_name,
  CASE
    WHEN c.cluster_name LIKE 'job-%' THEN 'Job Cluster'
    WHEN c.cluster_name LIKE 'dlt-%' THEN 'DLT Pipeline'
    ELSE 'Adhoc Cluster'
  END AS cluster_type,
  CASE
    WHEN c.cluster_name LIKE 'job-%' THEN regexp_extract(c.cluster_name, '^job-(\\d+)-')
    WHEN c.cluster_name LIKE 'dlt-%' THEN regexp_extract(c.cluster_name, '^dlt-(\\d+)-')
    ELSE NULL
  END AS job_or_pipeline_id,
  c.owned_by,
  c.cluster_id,
  c.create_time AS cluster_create_time,
  c.delete_time AS cluster_delete_time,
  CASE WHEN c.delete_time IS NOT NULL AND c.cluster_id IS NOT NULL THEN 1 ELSE 0 END AS IsClusterDeleted,
  c.tags AS cluster_tags,
  c.cluster_source
FROM system.compute.clusters c
QUALIFY (ROW_NUMBER() OVER (PARTITION BY cluster_id ORDER BY change_time DESC) = 1)
),
 
px_all AS (
  SELECT DISTINCT
  sku_name,
  pricing.default AS unit_price,
  unit_price::decimal(10,3) AS sku_price
  FROM system.billing.list_prices 
  QUALIFY ROW_NUMBER() OVER (PARTITION BY sku_name ORDER BY price_start_time DESC) = 1
  ),

clean_usage AS (
-- Final Select
SELECT 
u.*,
c.*,
u.workspace_id AS clean_workspace_id,
sku_price*usage_quantity AS Dollar_DBUs_List,
-- Clean up cluster / warehouse ids from all places
COALESCE(c.cluster_id, u.usage_metadata.cluster_id) AS clean_cluster_id,
COALESCE(u.usage_metadata.warehouse_id) AS clean_warehouse_id,
COALESCE(u.usage_metadata.job_id, u.usage_metadata.dlt_pipeline_id, c.job_or_pipeline_id,  NULL) AS clean_job_or_pipeline_id,
map_zip_with(
  IFNULL(u.custom_tags, map()),
  IFNULL(c.cluster_tags, map()),
  (k, v1, v2) -> COALESCE(v1, v2)
) AS clean_tags,
-- Compute Type
COALESCE(u.identity_metadata.run_as::string, c.owned_by) AS clean_usage_owner,
--Job Type (DLT / Jobs / )
CASE WHEN u.sku_name LIKE ('%SERVERLESS%') OR u.product_features.is_serverless = 'true' THEN 'Serverless' ELSE 'Self-hosted' END AS IsServerless

FROM system.billing.usage AS u
INNER JOIN px_all AS px ON px.sku_name = u.sku_name
LEFT JOIN compute AS c
    ON (c.cluster_id = u.usage_metadata.cluster_id
   --     AND (   (c.job_or_pipeline_id = u.usage_metadata.job_id)
   --             OR (c.job_or_pipeline_id = u.usage_metadata.dlt_pipeline_id)
   --             OR (c.job_or_pipeline_id = 'Adhoc')
   --         )
    )
)

SELECT * FROM clean_usage;



CREATE TABLE IF NOT EXISTS clean_usage_table
TBLPROPERTIES('delta.targetFileSize' = '16mb')
AS (
WITH compute AS (
  -- Optional table version, you can also use the MV version
SELECT
  c.cluster_name,
  CASE
    WHEN c.cluster_name LIKE 'job-%' THEN 'Job Cluster'
    WHEN c.cluster_name LIKE 'dlt-%' THEN 'DLT Pipeline'
    ELSE 'Adhoc Cluster'
  END AS cluster_type,
  CASE
    WHEN c.cluster_name LIKE 'job-%' THEN regexp_extract(c.cluster_name, '^job-(\\d+)-')
    WHEN c.cluster_name LIKE 'dlt-%' THEN regexp_extract(c.cluster_name, '^dlt-(\\d+)-')
    ELSE NULL
  END AS job_or_pipeline_id,
  c.owned_by,
  c.cluster_id,
  c.create_time AS cluster_create_time,
  c.delete_time AS cluster_delete_time,
  CASE WHEN c.delete_time IS NOT NULL AND c.cluster_id IS NOT NULL THEN 1 ELSE 0 END AS IsClusterDeleted,
  c.tags AS cluster_tags,
  c.cluster_source
FROM system.compute.clusters c
QUALIFY (ROW_NUMBER() OVER (PARTITION BY cluster_id ORDER BY change_time DESC) = 1)
),
 
px_all AS (
  SELECT DISTINCT
  sku_name,
  pricing.default AS unit_price,
  unit_price::decimal(10,3) AS sku_price
  FROM system.billing.list_prices 
  QUALIFY ROW_NUMBER() OVER (PARTITION BY sku_name ORDER BY price_start_time DESC) = 1
  ),

clean_usage AS (
-- Final Select
SELECT 
u.*,
c.*,
u.workspace_id AS clean_workspace_id,
sku_price*usage_quantity AS Dollar_DBUs_List,
-- Clean up cluster / warehouse ids from all places
COALESCE(c.cluster_id, u.usage_metadata.cluster_id) AS clean_cluster_id,
COALESCE(u.usage_metadata.warehouse_id) AS clean_warehouse_id,
COALESCE(u.usage_metadata.job_id, u.usage_metadata.dlt_pipeline_id, c.job_or_pipeline_id,  NULL) AS clean_job_or_pipeline_id,
map_zip_with(
  IFNULL(u.custom_tags, map()),
  IFNULL(c.cluster_tags, map()),
  (k, v1, v2) -> COALESCE(v1, v2)
) AS clean_tags,
-- Compute Type
COALESCE(u.identity_metadata.run_as::string, c.owned_by) AS clean_usage_owner,
--Job Type (DLT / Jobs / )
CASE WHEN u.sku_name LIKE ('%SERVERLESS%') OR u.product_features.is_serverless = 'true' THEN 'Serverless' ELSE 'Self-hosted' END AS IsServerless

FROM system.billing.usage AS u
INNER JOIN px_all AS px ON px.sku_name = u.sku_name
LEFT JOIN compute AS c
    ON (c.cluster_id = u.usage_metadata.cluster_id
   --     AND (   (c.job_or_pipeline_id = u.usage_metadata.job_id)
   --             OR (c.job_or_pipeline_id = u.usage_metadata.dlt_pipeline_id)
   --             OR (c.job_or_pipeline_id = 'Adhoc')
   --         )
    )
)

SELECT * FROM clean_usage
)
CLUSTER BY (usage_start_time);


DROP FUNCTION IF EXISTS main.dash_observability_advisor.generate_alert_info_from_prompt;

CREATE OR REPLACE FUNCTION main.dash_observability_advisor.generate_alert_info_from_prompt( input_prompt STRING, endpoint_name STRING DEFAULT 'databricks-dbrx-instruct')
RETURNS TABLE(result STRING)
RETURN 
SELECT 
ai_query('databricks-dbrx-instruct',
CONCAT('You are an advisor on Databricks for system table observability. Your job is to take in a user question on Databricks system tables and convert the users prompt into an alert for Databricks SQL. A Databricks SQL alert has the following output structure: QUERY: The SQL query that ALWAYS returns some BOOLEAN condition flag with a column ALWAYS named alert_condition. The SQL query alert must always evaluate to be a single record. If the SQL query is a time series query, make the alert select the most recent to evaluate. SCHEDULE: a quartz cron syntax to determine how often to run the SQL query to evaluate the alert. It MUST be quartz cron and not regular cron syntax.  The schedule can be empty, this means it will just be manually run. RECIPIENT: The alert should go to a recipient or list of recipients. That recipient should be the logged in user if not specified, or it should be a specific email. CONTEXT_SQL: This is a list of SQL queries that you can generate to ask for more context about specific data in a column. For example, if someone asks to generate an alert about usage with a custom_tag.key field with the demo tag, you can ask to get the values of that column by writing SQL to get that and I will provide the result. The available table you can query is called clean_usage, and the table schema you have to query looks like this in a JSON array: ', 
"[
    {'col_name': 'account_id', 'data_type': 'string'},
    {'col_name': 'workspace_id', 'data_type': 'string'},
    {'col_name': 'record_id', 'data_type': 'string'},
    {'col_name': 'sku_name', 'data_type': 'string', 'comment': 'this is the more specific version of billing_product_origin column.'},
    {'col_name': 'cloud', 'data_type': 'string', 'comment': 'AWS, Azure, GCP'},
    {'col_name': 'usage_start_time', 'data_type': 'timestamp'},
    {'col_name': 'usage_end_time', 'data_type': 'timestamp'},
    {'col_name': 'usage_date', 'data_type': 'date'},
    {'col_name': 'custom_tags', 'data_type': 'map<string,string>', comment: ' You can access custom_tag keys like this: custom_tags.demo'},
    {'col_name': 'usage_unit', 'data_type': 'string'},
    {'col_name': 'usage_quantity', 'data_type': 'decimal(38,18)'},
    {'col_name': 'usage_metadata', 'data_type': 'struct<cluster_id:string,job_id:string,warehouse_id:string,instance_pool_id:string,node_type:string,job_run_id:string,notebook_id:string,dlt_pipeline_id:string,endpoint_name:string,endpoint_id:string,dlt_update_id:string,dlt_maintenance_id:string>'},
    {'col_name': 'identity_metadata', 'data_type': 'struct<run_as:string>'},
    {'col_name': 'record_type', 'data_type': 'string'},
    {'col_name': 'ingestion_date', 'data_type': 'date'},
    {'col_name': 'billing_origin_product', 'data_type': 'string', 'comment': 'This field is related to the type of billing usage. Synonynms could be SKU, product category, etc. Sample values include SQL, ALL_PURPOSE, MODEL_SERVING, DLT.'},
    {'col_name': 'product_features', 'data_type': 'struct<jobs_tier:string,sql_tier:string,dlt_tier:string,is_serverless:boolean,is_photon:boolean,serving_type:string>'},
    {'col_name': 'usage_type', 'data_type': 'string'},
    {'col_name': 'cluster_name', 'data_type': 'string'},
    {'col_name': 'cluster_type', 'data_type': 'string'},
    {'col_name': 'job_or_pipeline_id', 'data_type': 'string'},
    {'col_name': 'owned_by', 'data_type': 'string'},
    {'col_name': 'cluster_id', 'data_type': 'string'},
    {'col_name': 'cluster_create_time', 'data_type': 'timestamp'},
    {'col_name': 'cluster_delete_time', 'data_type': 'timestamp'},
    {'col_name': 'IsClusterDeleted', 'data_type': 'int'},
    {'col_name': 'cluster_tags', 'data_type': 'map(string,string)'},
    {'col_name': 'cluster_source', 'data_type': 'string'},
    {'col_name': 'clean_workspace_id', 'data_type': 'string'},
    {'col_name': 'Dollar_DBUs_List', 'data_type': 'decimal(38,10)'},
    {'col_name': 'clean_cluster_id', 'data_type': 'string'},
    {'col_name': 'clean_warehouse_id', 'data_type': 'string'},
    {'col_name': 'clean_job_or_pipeline_id', 'data_type': 'string'},
    {'col_name': 'clean_tags', 'data_type': 'map(string,string)', comment: 'This is the parent and BEST column for getting tags. You can access custom_tag keys like this: custom_tags.demo'}},
    {'col_name': 'clean_usage_owner', 'data_type': 'string'},
    {'col_name': 'IsServerless', 'data_type': 'string'}
]",
" You can access custom_tag keys like this: custom_tags.demo or custom_tags['demo'] but not custom_tags['keys'] = 'demo'. Some example CONTEXT_SQL could include getting the distinct values in a column such as: SELECT DISTINCT custom_tags['demo'] FROM clean_usage WHERE usage_date = DATE_SUB(CURRENT_DATE, 1) \n",
" ALWAYS make the single output column name be 'alert_condition'. Generate a JSON ONLY output MUST be in the following JSON encodable format: {\"ALERT_NAME\": \"make an alert name based on the question to id the request\", \"QUERY\": \"sql_query\", \"SCHEDULE\": \"quartz cron schedule\", \"RECIPIENTS\": [\"list of recipients databricks user names\"], \"CONTEXT_SQL\": [\"list of SQL statement to execute for more context to answer the quesiton\"]} Lastly, if you need information about specific values inside the dataframe to answer the question, then please generate the SQL (or a list of SQL statements if you need multiple context columns), and pass it into the \"CONTEXT_SQL\" key result. Once you get all the context you need, mark your answer FINAL. DO NOT INSERT COMMENTS exlaining your JSON. Use the following input prompt as guidance: INPUT_PROMPT:",
input_prompt, ".")
) AS result;